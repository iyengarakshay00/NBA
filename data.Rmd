# Data 

```{r}
# remotes::install_github("jtr13/redav")
library(redav)
```

## Sources

### Season data
Our first data source was ESPN, which we used for aggregate team performance across several years. ESPN offers no convenient way to download data, so we resorted to scraping. We needed to make sure that webscraping the data from the site was legal and ethical. That is why we didn't edit any of the numbers and made sure the data was in the correct format. After using a library called Rvest, we were able to grab both western and eastern conferences as a table. Then, we repeated this for multiple seasons in order to get data for all teams and growth across the years. Some of the problems with the table was that it wasn't in order like the ESPN by position. The table wasn't combined and there wasn't indicators for years or conferences. The teams also were in a format that wouldn't be able to be grouped in a graph. For example, the team was like "Milwakee Bucksz" which is different than the team name because it had the letter or position combined. This took some cleaning and transformation, which we discuss below. There were about 90 records considering 3 seasons and 30 NBA teams. Some of the variables are team name, team name adjusted (which is a clean abbreviated name for team), conference type (western or eastern), year (when season started), and more. Some of the numerical variables dealt with points per game, opponent points per game, wins, and losses.

### Shot data
We also pulled granular shot data directly from the NBA's public APIs for games from 2017 to 2022. The NBA itself collects and publishes this data. They make it available for use (according to their [Terms of Use](https://www.nba.com/termsofuse)) so long as it is not used for profit, and as long as NBA.com is given attribution. Consider this that attribution, so usage of this data is permissible for this project.

The NBA offers a substantial number of APIs for accessing different statistics. We used a [python package](https://github.com/swar/nba_api) to help access those APIs. Specifically, we downloaded two datasets using this API:
- shot-by-shot locations (as in, place on the basketball court). Each row reflects one shot, along with identifying columns like who shot it, their team at the time, how far it was from the hoop, date, and season. This is a substantial dataset with nearly 500k rows and over 50mb in size.
- aggregated total number of shots by player and season. This is a small dataset since it's aggregated; no more than 3k rows.


## Cleaning / transformation

### Season data

Scraping and cleaning is shown for this dataset in `data/pull/Season.R`. Getting this data into a tabular csv form was a lengthy process. Extracting using rvest gets the table into separated table values. There were 4 tables for one year using rvest. Each table represented one of two things either a list of teams or the list of variable values related to the team for the season performance per conference. We had to join the the teams with the list of values using its row names. Later on, row names was converted into an integer so we could have a position of the team within the respective conference. We also added a category column to indicate whether eastern or western conference something that might be useful to denote. Once that was done, the eastern and western conference tables were merged into one using a union command. We added a year, which would be converted to an integer to denote the start of the season for that table. This was repeated for the next two following seasons. In order to get a big table full of all 3 seasons we union_all the data frames to get all value combinations for the final data frame for season performances. A unique thing while looking at the data set was the name for the team was combined with positions/bye performances, which were denoted by special letters such as y or z. In order to clean this, we created a column called ACT_TEAM, which is an abbreviation of the letters of the team. Using a regular expression formula, we were able to extract the 3 NBA letter. Now, not everything can be accurate to the NBA abbreviation, which is why human oversight over data is so important. We were able to adapt and change 4 of the abbreviations to match the NBA ones.


### Shot data

Extracting the shot data took a substantial amount of time and effort, so we wanted to explicitly note that time commitment here. There were several reasons for this:
- The Python library, while helpful relative to the NBA's janky API, was poorly documented.
- We discovered that the NBA's API has some secret rate-blocking mechanism to prevent DDOS. We figured this out after getting our IP address temporarily blocked by their servers. After resolving this, we learned that this meant in practice we had to sleep one second between API calls. This doesn't sound like much of a problem, but the API to extract granular player shots requires one call per player and season. There are several hundreds of players each season, and each API call also might take several seconds itself, so it would have taken many hours to get the entirety of shot data. To resolve this, we ended up only pulling the shots of the 100 players who shot the most each season. This top-shooting-player information was determined by the data in `data/files/fga_by_player_2017-22.csv` (pulled via `data/pull/fga_by_player.py`). Then we extracted shot data via `data/pull/shot_data.py` and stored it in `data/files/shot_data_2017-22.csv`. This is still a substantial amount of data, and we provide justification for this approach later in Results -- long story short, this allows us to analyze roughly 50% of shot data without wasting hours pulling the data.

On the other hand, this data did not require much cleaning / transformation, since we did most manipulation at pull-time as shown in the Python scripts; it only involved things like selecting and renaming columns. In R before visualizing, we also did a few simple things to coerce column types and appropriately label the different play periods (pre, during, and post-bubble) so we could visualize those periods easily.

## Missing value analysis

### Season data
Below shows missing values for one of the seasonal data tables. It has no missing data as you can see; there is a complete case file for all the rows. This is because when getting the ESPN data, it contains all of the statistics for the seasons for every team. This is definitely one of the perks of getting data directly from the source itself.

```{r}
df_all = read.csv('./data/files/Season.csv')
plot_missing(df_all, percent = FALSE)
```

### Shot data
Like above, our shot dataset also contained no missing data:
```{r}
shots <- read.csv('data/files/shot_data_2017-2022.csv')
plot_missing(shots)
```
